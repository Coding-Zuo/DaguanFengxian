# 达观——大规模预训练模型的风险事件标签识别

# 使用bert

- 脱敏数据转明文的版本
- bert embedding替换脱敏数据的随机embedding的
- bert embedding替换脱敏数据的word2vec embedding的

## bert_base_0_split 分数：0.52042886
python training/main.py --model_type bert --model_name_or_path /home/zuoyuhui/DataGame/haihuai_RC/chinese-bert-wwm-ext  --data_dir /data2/code/DaguanFengxian/bert_model/data/splits/fold_0_bertvocab  --label_file_level_1 /data2/code/DaguanFengxian/bert_model/data/labels_level_1.txt --label_file_level_2 /data2/code/DaguanFengxian/bert_model/data/labels_level_2.txt --task daguan --aggregator bert_pooler --model_dir /data2/code/DaguanFengxian/bert_model/data/outputs/bert_base_bertvocab  --do_train --do_eval --train_batch_size 16 --num_train_epochs 50 --embeddings_learning_rate 0.4e-4 --encoder_learning_rate 0.5e-4 --classifier_learning_rate 5e-4 --warmup_steps 200 --max_seq_len 132 --dropout_rate 0.15 --metric_key_for_early_stop "macro avg__f1-score__level_2" --logging_steps 200 --patience 12 --label2freq_level_1_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_1.json --label2freq_level_2_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_2.json


## bert_large_0split 分数：0.51709728479 
python training/main.py --model_type bert --model_name_or_path /data2/pre-model/bert/bert_large_chinese --data_dir /data2/code/DaguanFengxian/bert_model/data/splits/fold_1_bert_large_vocab --label_file_level_1 /data2/code/DaguanFengxian/bert_model/data/labels_level_1.txt --label_file_level_2 /data2/code/DaguanFengxian/bert_model/data/labels_level_2.txt --task daguan --aggregator bert_pooler --model_dir /data2/code/DaguanFengxian/baseline2/data/outputs/bert_large_1 --do_train --do_eval --train_batch_size 16 --gradient_accumulation_steps 1 --num_train_epochs 50 --embeddings_learning_rate 0.4e-4 --encoder_learning_rate 0.5e-4 --classifier_learning_rate 5e-4 --warmup_steps 200 --max_seq_len 132 --dropout_rate 0.15 --metric_key_for_early_stop "macro avg__f1-score__level_2" --logging_steps 200 --patience 12 --label2freq_level_1_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_1.json --label2freq_level_2_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_2.json --eval_batch_size 16

## nezha-base-wwm 分数：0.49205413965 dice
CUDA_VISIBLE_DEVICES="1" python src/bert_models/training/main.py --model_type nezha --model_name_or_path /data2/pre-model/nezha/NEZHA-Base-WWM --data_dir /data2/code/DaguanFengxian/bert_model/data/splits/fold_0_nezha_base_vocab --label_file_level_1 /data2/code/DaguanFengxian/bert_model/data/labels_level_1.txt --label_file_level_2 /data2/code/DaguanFengxian/bert_model/data/labels_level_2.txt  --task daguan --aggregator bert_pooler --model_dir /data2/code/DaguanFengxian/04课/src/bert_models/data/outputs/train.nezha_base_lr0.7e-4_dice_bs64_v2 --do_train --do_eval --train_batch_size 64 --num_train_epochs 50 --embeddings_learning_rate 0.7e-4 --encoder_learning_rate 0.7e-4 --classifier_learning_rate 7e-4 --warmup_steps 200 --max_seq_len 132 --dropout_rate 0.15 --metric_key_for_early_stop "macro avg__f1-score__level_2" --logging_steps 200 --patience 6 --label2freq_level_1_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_1.json --label2freq_level_2_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_2.json --processor_sep "\t" --loss_fct_name dice 


## nezha-base-wwm 分数：0.50174915911  dice
CUDA_VISIBLE_DEVICES="1" python src/bert_models/training/main.py --model_type nezha --model_name_or_path /data2/pre-model/nezha/NEZHA-Base --data_dir /data2/code/DaguanFengxian/bert_model/data/splits/fold_0_nezha_base_vocab --label_file_level_1 /data2/code/DaguanFengxian/bert_model/data/labels_level_1.txt --label_file_level_2 /data2/code/DaguanFengxian/bert_model/data/labels_level_2.txt  --task daguan --aggregator bert_pooler --model_dir /data2/code/DaguanFengxian/04/src/bert_models/data/outputs/train.nezha_base_nowwm_lr0.7e-4_dice_bs64_v2 --do_train --do_eval --train_batch_size 64 --num_train_epochs 50 --embeddings_learning_rate 0.7e-4 --encoder_learning_rate 0.7e-4 --classifier_learning_rate 7e-4 --warmup_steps 200 --max_seq_len 132 --dropout_rate 0.15 --metric_key_for_early_stop "macro avg__f1-score__level_2" --logging_steps 200 --patience 6 --label2freq_level_1_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_1.json --label2freq_level_2_dir /data2/code/DaguanFengxian/bert_model/data/label2freq_level_2.json --processor_sep "\t" --loss_fct_name dice 


## Results

模型结果记录: 采用5折交叉验证的平均分

| 模型描述                                    | dev macro-F1        | 线上得分        |
| ------------------------------------------- | ------------------- |------------------- |
| 官方baseline：   word2vec + bilstm          | 0.392               |               |
| 随机word2vec + bilstm +  max-pool           | 0.491               |              |
| - + slf_attn_pool                           | 0.49267002199706494 |              |
| 预训练word2vec + bilstm +  max-pool         | 0.5013160507998644  |              |
| -  +  slf_attn_pool                         | 0.4972518334336293  |              |
| BERT-base + 随机初始化embedding             | 0.505               |              |
| - + 词汇表词频对应                          | 0.515               |              |
| BERT-wwm-ext + 词汇表词频对应               | 0.524               |              |
| -   多种pooling操作一起使用                 | 0.528               |              |
| - + sample weights                          | 0.535               |              |
| - + ce + NTXENT loss                        | 0.533               |              |
| NEZHA-base-wwm +  词汇表词频对应            | 0.535               |              |
| - +  ce + NTXENT loss (系数0.1, gamma 0.5)  | 0.538               |              |
| - +  ce + NTXENT loss (系数0.5, gamma 0.5)  | 0.530               |              |
| - +  ce + NTXENT loss (系数0.5, gamma 0.07) | 0.526               |              |
| - +  multi-sample dropout (num=4, sum)      | 0.543               |              |

| 模型描述                                          | dev macro-F1        | 线上得分        |  备注 |
| -------------------------------------------      | ------------------- |------------------- | ------------------- |
| nezha-base+ce+bert_pooler,dr_pooler,max_pooler   | 0.519  |               | train.nezha_base_v2    |

| nezha_base_wwm_ce_ntx_0.1_0.5_sa_dr_msdrop_sum_4_0.4__v3 |  0.527    | 0.5304    |     | 比较sa还是sace 第一轮sa比sace高1.2
| nezha_base_wwm_ce_ntx_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3 | 0.541   |               |     |
| nezha_base_wwm_ce_ntx_0.1_0.5_sace_dr_msdrop_avg_4_0.4__v3 | 0.525    |               |     |

| nezha_base_wwm_ce_ntx_0.1_0.5_sace_dr_msdrop_sum_8_0.4__v3 |   0.523   |               |     | 比较msdrop 4个还是8个 平均还是加和 加和第一轮好太多
| nezha_base_wwm_ce_ntx_0.1_0.5_sace_dr_msdrop_average_8_0.4__v3 | 0.535   | 0.5219  |     |

| nezha_base_wwm_focal_ntx_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3 | 0.549   |  0.5192      |     |对比学习与focal dice结合
| nezha_base_wwm_dice_ntx_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3 | 0.527  | 0.5444        |     |

| nezha_base_wwm_ce_supcon_0.1_0.5_sa_dr_msdrop_sum_4_0.4__v3 |   |               |     |
| nezha_base_wwm_focal_supcon_0.1_0.5_sa_dr_msdrop_sum_4_0.4__v3 |   |               |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3 | 0.5511、0.5279、0.5197   |  0.5489      |     |
| nezha_base_wwm_dice_supconloss_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi__v3 | 0.5017、0.523、0.532   | 0.5156   |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_fgm_grulstm__v3 |   |    |     |

| nezha_base_wwm_dice_supconloss_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_fgm_lstmgru_v3 |  0.5466   |  0.5349     |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_gru_v3 | 0.535 |   |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_lstm_v3 |  | 0.5306 |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_lstmgru_v3 | 0.5403 |   |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_lstmgru_multi_v3 | 0.5403 |   |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_fgm_grulstm_v3 | 0.5353 |     |     |

| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan0.4_v3 | 0.493    |        |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan0.2_v3 | 0.5511  | 0.5489  |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan0.2_v3_fold1 | 0.5169  |   |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan0.2_v3_fold2 | 0.516  |   |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan0.2_v3_fold3 | 0.516 |   |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan0.2_v3_fold4 | 0.538 |   |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan0.2_fgm0.05_v3 |0.5404 |   |     |

| nezha_base_wwm_dice_ntx_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3_fgm | 0.5270   |               |     |
| nezha_base_wwm_dice_ntx_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3_pgd |  0.536  | 0.5178   |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3_fgm0.03 | 0.547   | 0.5353      |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3_fgm0.1 | 0.5492 | 0.5207  |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3_pgd | 0.544 | 0.5116    |     |

| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold1__v3 | 0.5436 |     |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold2__v3 | 0.5322 |     |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold3__v3 | 0.5223 |     |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold4__v3 | 0.5331 |     |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold5__v3 |  |     |     |

| bert120k_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3 |    |        |     |
| bert120k_focal_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm__v3 | 0.531   |        |     |

| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4__v3 | 0.5543 | 0.5329    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi__v3 | 0.5376 |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_grulstm__v3 | 0.5543  |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm__v3 |   |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fgm |  0.5473 | 0.5497    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fgm__v3 | 0.5588 |  0.5447   |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_pgd__v3 | 0.5285  |    |     |
| bert120k_ce_dr_msdrop_sum_4_0.4__v3(错误) | 0.518 |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm__v3 | 0.5506  | 0.5584   |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_swa__v3 | 0.5499  | 0.5255   |     |

| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold1__v3 | 0.5486  |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold2__v3 | 0.5483 |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold3__v3 | 0.5447 |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold4__v3 | 0.5648 | 0.56086   |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_nohongfan_grulstm_fold4__v3 | 0.5483 |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold5__v3 | 0.5292 |    |     |

| albert_base_ce_supcon_0.1_0.5_sa_dr_msdrop_sum_4_0.4__v3| 0.5005 | 0.5242   |     |

# 新五折
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold0__v3 | 0.5371 |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold1__v3 | 0.5379 |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold2__v3 | 0.5548 |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold3__v3 | 0.5594 |    |     |
| bert120k_ce_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_multi_hongfan_grulstm_fold4__v3 | 0.5492 |    |     |

| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold0__v3 | 0.5395  |    |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold1__v3 | 0.5284  |    |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold2__v3 | 0.5210 |    |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold3__v3 | 0.5229 |    |     |
| nezha_base_wwm_dice_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_grulstm_fold4__v3 | 0.5609 |    |     |

| albert_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_lstmgru__v3_fold0 | 0.5045 |    |     |
| albert_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_lstmgru__v3_fold1 | 0.4544 |    |     |
| albert_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_lstmgru__v3_fold2 | 0.4307 |    |     |
| albert_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_lstmgru__v3_fold3 | 0.4944 |    |     |

| bert_base_wwm_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_lstmgru_fold0__v3 | 0.5262 |    |     |
| bert_base_wwm_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_lstmgru_fold1__v3 | 0.5473 |    |     |
| bert_base_wwm_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_lstmgru_fold2__v3 | 0.5258 |    |     |
| bert_base_wwm_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_lstmgru_fold3__v3 | 0.5020 |    |     |
| bert_base_wwm_supcon_0.1_0.5_sace_dr_msdrop_sum_4_0.4_hongfan_lstmgru_fold4__v3 | 忘记录 |    |     |



| vote_first |    |  0.5809   |     |
| vote_two |    |  0.5618   |     |
| argmax_first |    |  0.5549   |     |

| vote_three |    |  0.584   |     |
| vote_three_rank |    | 0.58524   |     |
| vote_three_prob |    |  0.58507   |     |
| vote_three_avg |    | 0.574036   |     |

